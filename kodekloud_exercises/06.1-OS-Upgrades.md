## Questions

### 1) 
<details> 
  <summary markdown="span">Answer</summary>


    root@controlplane:~# COUNT="grep -vc NAME"
    root@controlplane:~# alias kg="k get"
    root@controlplane:~# k get nodes | grep -vc NAME
    2
</details>

### 2) How many applications do you see hosted on the cluster?
Check the number of deployments
<details>
  <summary markdown="span">Answer</summary>

    kg deploy | $COUNT
    1    
</details>

### 3) Which nodes are the applications hosted on?
<details>
  <summary markdown="span">Answer</summary>

    ==> node01

    root@controlplane:~# kg pods -owide -l app=blue
    NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES
    blue-746c87566d-4dmhd   1/1     Running   0          3m56s   10.244.1.2   node01   <none>           <none>
    blue-746c87566d-9hdbn   1/1     Running   0          3m56s   10.244.1.3   node01   <none>           <none>
    blue-746c87566d-j4c9g   1/1     Running   0          3m56s   10.244.1.4   node01   <none>           <none>
</details>

### 4) We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable.
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# k drain nodes/node01
    node/node01 cordoned
    error: unable to drain node "node01", aborting command...
    
    There are pending nodes to be drained:
     node01
    error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-zpnx9, kube-system/kube-proxy-df29p
    root@controlplane:~# k get ds
    No resources found in default namespace.

    root@controlplane:~# k drain nodes/node01 --ignore-daemonsets
    node/node01 already cordoned
    WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-zpnx9, kube-system/kube-proxy-df29p
    evicting pod default/blue-746c87566d-9hdbn
    evicting pod default/blue-746c87566d-j4c9g
    evicting pod default/blue-746c87566d-4dmhd
    pod/blue-746c87566d-9hdbn evicted
    pod/blue-746c87566d-j4c9g evicted
    pod/blue-746c87566d-4dmhd evicted
    node/node01 evicted
</details>

### 5) What nodes are the apps on now?
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# kd pods | grep Node:
    Node:         controlplane/10.34.99.12
    Node:         controlplane/10.34.99.12
    Node:         controlplane/10.34.99.12
</details>

### 6) The maintenance tasks have been completed. Configure the node node01 to be schedulable again.
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# k uncordon node/node01
    node/node01 uncordoned
</details>

### 7) How many pods are scheduled on node01 now? (NOTE: Should say in default namespace)
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# kd pods | grep Node:   
    Node:         controlplane/10.34.99.12
    Node:         controlplane/10.34.99.12
    Node:         controlplane/10.34.99.12
    root@controlplane:~# kd pods | grep Node: | grep -c node01
    0

</details>

### 8) Why are there no pods on node01?
1) Only when new pods are created they will be scheduled
2) node01 is cordoned
3) node01 did not upgrade successfully
4) node01 is faulty 
<details>
  <summary markdown="span">Answer</summary>

    ==> Only when new podsa are created they will be scheduled 

</details>

### 9) Why are the pods placed on the controlplane node? Check the master/controlplane node details
1) master/controlplane node has taints set on it
2) master/controlplane node is cordoned
3) you can never have pods on master nodes
4) controlplane node does not have any taints
5) master/controlplane node is faulty
<details>
  <summary markdown="span">Answer</summary>

    ==> #4) controlplane node does not have any taints

    root@controlplane:~# kd node/controlplane | grep "Unschedulable\|Taints"
    Taints:             <none>
    Unschedulable:      false
</details>

### 10) Maintenance Window 


### 11) We need to carry out a maintenance activity on node01 again. Try draining the node again using the same command as before: kubectl drain node01 --ignore-daemonsets
Did that work?
1) Yes
2) No
<details>
  <summary markdown="span">Answer</summary>

    ==> #2 No

    root@controlplane:~# k drain node/node01 --ignore-daemonsets 
    node/node01 cordoned
    error: unable to drain node "node01", aborting command...
    
    There are pending nodes to be drained:
     node01
    error: cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override): default/hr-app
</details>

### 12) Why did the drain command fail on node01? It worked the first time!
1) node01 tainted
2) there is a pod in node01 which is not part of a replicaset
3) no pods on node01
4) node01 was not upgraded correctly the last time
<details>
  <summary markdown="span">Answer</summary>

    ==> #2 there is a pod in node01 which is not part of a replicaset

    root@controlplane:~# kd node/node01 | sed -n '/Non-terminated/,/Allocated/p'
    Non-terminated Pods:          (3 in total)
      Namespace                   Name                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
      ---------                   ----                     ------------  ----------  ---------------  -------------  ---
      default                     hr-app                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m29s
      kube-system                 kube-flannel-ds-zpnx9    100m (0%)     100m (0%)   50Mi (0%)        300Mi (0%)     62m
      kube-system                 kube-proxy-df29p         0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m
    Allocated resources:
</details>

### 13) What is the name of the POD hosted on node01 that is not part of a replicaset?
1) redis
2) hr-app
3) red
4) simple-webapp-1
5) blue
<details>
  <summary markdown="span">Answer</summary>

    ==> #2) hr-app
</details>

### 14) What would happen to hr-app if node01 is drained forcefully?
Try it and see for yourself.
1) hr-app will be recreated on other nodes
2) hr-app will be re-created on master
3) hr-app will continue to run as a Docker container
4) hr-app will be lost forever
<details>
  <summary markdown="span">Answer</summary>

    ==> #4) hr-app will be lost forever

    root@controlplane:~# k drain node/node01 --ignore-daemonsets --force
    node/node01 already cordoned
    WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: default/hr-app; ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-zpnx9, kube-system/kube-proxy-df29p
    evicting pod default/hr-app
    pod/hr-app evicted
    node/node01 evicted

    root@controlplane:~# k get pods -A | grep -c hr
    0
</details>  

### 15) Oops! We did not want to do that! hr-app is a critical application that should not be destroyed. We have now reverted back to the previous state and re-deployed hr-app as a deployment.
<details>
  <summary markdown="span">Answer</summary>

</details>

### 16) hr-app is a critical app and we do not want it to be removed and we do not want to schedule any more pods on node01.
Mark node01 as unschedulable so that no new pods are scheduled on this node.
Make sure that hr-app is not affected.
- Node01 Unschedulable
- hr-app still running on node01?
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# k get pods -owide | grep 'hr\|NAME'
    NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
    hr-app-76d475c57d-fqscj   1/1     Running   0          80s   10.244.1.6   node01         <none>           <none>
    root@controlplane:~# k drain node/node01 --ignore-daemonsets
    node/node01 cordoned
    WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-zpnx9, kube-system/kube-proxy-df29p
    evicting pod default/hr-app-76d475c57d-fqscj
    
    pod/hr-app-76d475c57d-fqscj evicted
    node/node01 evicted

    root@controlplane:~# kg pods -owide | grep 'hr\|NAME'
    NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
    hr-app-76d475c57d-zvl26   1/1     Running   0          66s   10.244.1.7   node01         <none>           <none>
    root@controlplane:~# kd node/node01 | grep Unschedulable
    Unschedulable:      true

    ??? Should have drained??
</details>

### 17)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 18)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 19)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 20)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 21)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 22)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 23)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 24)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 25)
<details>
  <summary markdown="span">Answer</summary>

</details>

## TABLE TEMPLATES


### 2-Col | w/ Steps
| STEP  |               |               |       
| ----- | ------------- | ------------- |
| 1     |               |               |               
| 2     |               |               |               
| 3     |               |               |               
| 4     |               |               |               
| 5     |               |               |               
| 6     |               |               |               
| 7     |               |               |               
| 8     |               |               |               
| 9     |               |               |               
| 10    |               |               |               


### 2-Col | w/ Steps
|       |               |               |       
| ----- | ------------- | ------------- |
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |  


## #) | Section
1)
2)
3)
4)
5)
6)
7)
8)
9)
10) 
