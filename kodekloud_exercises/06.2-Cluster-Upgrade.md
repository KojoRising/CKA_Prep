## Questions

### This lab tests your skills on upgrading a kubernetes cluster. We have a production cluster with applications running on it. Let us explore the setup first.


### Summary Questions
1) Post "k drain", how can you see if the node is Unschedulable? Are any taints applied?
   
        root@controlplane:~# kd node/controlplane | grep "Taints\|Unschedulable"
        Taints:             node.kubernetes.io/unschedulable:NoSchedule
        Unschedulable:      true

### Pre-Lab
    COUNT="grep -vc NAME"
    FIND="grep --color=always -e "^" -e"
    D="--dry-run=client -oyaml"
    alias km=kubeadm
    alias k=kubectl
    alias kg="k get"
    alias kd="k describe"
    alias kl="k logs"
    alias kla="k label"
    alias kx='k exec -it'
    alias kc='k create'
    alias kcf="k create -f"
    alias kcd='k create $D'
    alias ke="k explain"
    alias kdrain="k drain --ignore-daemonsets --force"
    ker() { k explain $1 --recursive=true | grep '<' | sed 's/<.*//'; }
    complete -F __start_kubectl k


### 1) What is the current version of the cluster?
<details> 
  <summary markdown="span">Answer</summary>

    ==> v.19.0

    NOTE: I believe refers to the Kubelet Version

    root@controlplane:~# kg nodes
    NAME           STATUS   ROLES    AGE   VERSION
    controlplane   Ready    master   47m   v1.19.0
    node01         Ready    <none>   47m   v1.19.0

    root@controlplane:~# kd nodes | grep "Kubelet.*:"
    Kubelet Version:            v1.19.0
    Kubelet Version:            v1.19.0
</details>

### 2) How many nodes are part of this cluster?
Including master and worker nodes
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# kg nodes -A | $COUNT
    2
</details>

### 3) How many nodes can host workloads in this cluster? Inspect the applications and taints set on the nodes.
<details>
  <summary markdown="span">Answer</summary>

    ==> 2

    root@controlplane:~# kd nodes -A | grep "Taints"
    Taints:             <none>
    Taints:             <none>
</details>

### 4) How many applications are hosted on the cluster? Count the number of deployments.
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# kg deploy -Aowide | grep -vc "NAME\|kube-system"
    1
</details>

### 5) What nodes are the pods hosted on?
1) node01
2) master
3) node02
4) node01,node02
5) controlplane,node01
<details>
  <summary markdown="span">Answer</summary>

    ==> 5) controlplane,node01

    root@controlplane:~# kg pods -Aowide | grep -c controlplane 
    10
    root@controlplane:~# kg pods -Aowide | grep -c node
    6
</details>

### 6) You are tasked to upgrade the cluster. User's accessing the applications must not be impacted. And you cannot provision new VMs. What strategy would you use to upgrade the cluster?
1) Users will be impacted since there is only one worker node
2) Add new nodes with newer versions while taking down existing nodes
3) Upgrade all nodes at once
4) Upgrade one node at a time while moving the workloads to the other
<details>
  <summary markdown="span">Answer</summary>

    ==> 4) Upgrade one node at a time while moving the workloads to the other
</details>

### 7) What is the latest stable version available for upgrade?
Use the kubeadm tool
<details>
  <summary markdown="span">Answer</summary>

    ==> v1.19.14
    root@controlplane:~# km upgrade plan
    [upgrade/config] Making sure the configuration is correct:
    [upgrade/config] Reading configuration from the cluster...
    [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
    [preflight] Running pre-flight checks.
    [upgrade] Running cluster health checks
    [upgrade] Fetching available versions to upgrade to
    [upgrade/versions] Cluster version: v1.19.0
    [upgrade/versions] kubeadm version: v1.19.0
    I0829 20:51:33.648051   35378 version.go:252] remote version is much newer: v1.22.1; falling back to: stable-1.19
    [upgrade/versions] Latest stable version: v1.19.14
    [upgrade/versions] Latest stable version: v1.19.14
    [upgrade/versions] Latest version in the v1.19 series: v1.19.14
    [upgrade/versions] Latest version in the v1.19 series: v1.19.14
    
    Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
    COMPONENT   CURRENT       AVAILABLE
    kubelet     2 x v1.19.0   v1.19.14
    
    Upgrade to the latest version in the v1.19 series:
    
    COMPONENT                 CURRENT   AVAILABLE
    kube-apiserver            v1.19.0   v1.19.14
    kube-controller-manager   v1.19.0   v1.19.14
    kube-scheduler            v1.19.0   v1.19.14
    kube-proxy                v1.19.0   v1.19.14
    CoreDNS                   1.7.0     1.7.0
    etcd                      3.4.9-1   3.4.9-1
    
    You can now apply the upgrade by executing the following command:
    
            kubeadm upgrade apply v1.19.14
    
    Note: Before you can perform this upgrade, you have to update kubeadm to v1.19.14.
    
    _____________________________________________________________________
    
    
    The table below shows the current state of component configs as understood by this version of kubeadm.
    Configs that have a "yes" mark in the "MANUAL UPGRADE REQUIRED" column require manual config upgrade or
    resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually
    upgrade to is denoted in the "PREFERRED VERSION" column.
    
    API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED
    kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no
    kubelet.config.k8s.io     v1beta1           v1beta1             no
    _____________________________________________________________________

</details>

### 8) We will be upgrading the master node first. Drain the master node of workloads and mark it UnSchedulable
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# k drain node/controlplane
    node/controlplane cordoned
    error: unable to drain node "controlplane", aborting command...
    
    There are pending nodes to be drained:
     controlplane
    error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-jzlhd, kube-system/kube-proxy-zdrpt
    root@controlplane:~# k drain node/controlplane --ignore-daemonsets --force
    node/controlplane already cordoned
    WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-jzlhd, kube-system/kube-proxy-zdrpt
    evicting pod kube-system/coredns-f9fd979d6-55csf
    evicting pod kube-system/coredns-f9fd979d6-d6st2
    evicting pod default/blue-746c87566d-7rsk9
    evicting pod default/blue-746c87566d-lhgx8
    pod/blue-746c87566d-7rsk9 evicted
    pod/coredns-f9fd979d6-55csf evicted
    pod/blue-746c87566d-lhgx8 evicted
    pod/coredns-f9fd979d6-d6st2 evicted
    node/controlplane evicted
</details>

### 9) Upgrade the controlplane components to exact version v1.20.0
Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet. 
Practice referring to the kubernetes documentation page. 
Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead
<details>
  <summary markdown="span">Answer</summary>

    STEPS: 
    1) apt-get update
    2) apt list -a kubeadm
    3) apt install kubeadm=[VERSION] // From Above
    4) km upgrade plan 
        // NOTE: Upgraded version will change now:  
        > kubeadm upgrade apply v1.20.10
    4) kubeadm upgrade apply v1.20.10
    5) apt-get update &&  apt install kubelet=1.20.0

    ### NOTE - illegal zero-prefixed version component "00" in "v1.20.00"
    root@controlplane:~# kubeadm upgrade apply v1.20.00 
    ...
    couldn't parse Kubernetes version "v1.20.00": illegal zero-prefixed version component "00" in "v1.20.00"
    To see the stack trace of this error execute with --v=5 or higher


    ################ STEP 1 ################ 
    ################ STEP 2 ################ 
    ################ STEP 3 ################ 
    ################ STEP 4 ################ 
    root@controlplane:~# kubeadm upgrade apply v1.20.0
    [upgrade/config] Making sure the configuration is correct:
    [upgrade/config] Reading configuration from the cluster...
    [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
    [preflight] Running pre-flight checks.
    [upgrade] Running cluster health checks
    [upgrade/version] You have chosen to change the cluster version to "v1.20.0"
    [upgrade/versions] Cluster version: v1.19.0
    [upgrade/versions] kubeadm version: v1.20.0
    [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
    [upgrade/prepull] Pulling images required for setting up a Kubernetes cluster
    [upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection
    [upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'
    [upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.20.0"...
    Static pod: kube-apiserver-controlplane hash: 5e37cdbe6c112fd30dd8655bd75e30c6
    Static pod: kube-controller-manager-controlplane hash: f6a9bf2865b2fe580f39f07ed872106b
    Static pod: kube-scheduler-controlplane hash: 5146743ebb284c11f03dc85146799d8b
    [upgrade/etcd] Upgrading to TLS for etcd
    Static pod: etcd-controlplane hash: 96cf3b24dba2b2ae09f16dbc96aec84d
    [upgrade/staticpods] Preparing for "etcd" upgrade
    [upgrade/staticpods] Renewing etcd-server certificate
    [upgrade/staticpods] Renewing etcd-peer certificate
    [upgrade/staticpods] Renewing etcd-healthcheck-client certificate
    [upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/etcd.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-08-29-21-09-26/etcd.yaml"
    [upgrade/staticpods] Waiting for the kubelet to restart the component
    [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
    Static pod: etcd-controlplane hash: 96cf3b24dba2b2ae09f16dbc96aec84d
    Static pod: etcd-controlplane hash: 96cf3b24dba2b2ae09f16dbc96aec84d
    Static pod: etcd-controlplane hash: d38588cec61f27555e598b9f1a259b88
    [apiclient] Found 1 Pods for label selector component=etcd
    [upgrade/staticpods] Component "etcd" upgraded successfully!
    [upgrade/etcd] Waiting for etcd to become available
    [upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests864279684"
    [upgrade/staticpods] Preparing for "kube-apiserver" upgrade
    [upgrade/staticpods] Renewing apiserver certificate
    [upgrade/staticpods] Renewing apiserver-kubelet-client certificate
    [upgrade/staticpods] Renewing front-proxy-client certificate
    [upgrade/staticpods] Renewing apiserver-etcd-client certificate
    [upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-08-29-21-09-26/kube-apiserver.yaml"
    [upgrade/staticpods] Waiting for the kubelet to restart the component
    [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
    Static pod: kube-apiserver-controlplane hash: 5e37cdbe6c112fd30dd8655bd75e30c6
    Static pod: kube-apiserver-controlplane hash: 79f63a522d57e7516708609ec47322f4
    [apiclient] Found 1 Pods for label selector component=kube-apiserver
    [upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
    [upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
    [upgrade/staticpods] Renewing controller-manager.conf certificate
    [upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-08-29-21-09-26/kube-controller-manager.yaml"
    [upgrade/staticpods] Waiting for the kubelet to restart the component
    [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
    Static pod: kube-controller-manager-controlplane hash: f6a9bf2865b2fe580f39f07ed872106b
    Static pod: kube-controller-manager-controlplane hash: a875134e700993a22f67999011829566
    [apiclient] Found 1 Pods for label selector component=kube-controller-manager
    [upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
    [upgrade/staticpods] Preparing for "kube-scheduler" upgrade
    [upgrade/staticpods] Renewing scheduler.conf certificate
    [upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-08-29-21-09-26/kube-scheduler.yaml"
    [upgrade/staticpods] Waiting for the kubelet to restart the component
    [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
    Static pod: kube-scheduler-controlplane hash: 5146743ebb284c11f03dc85146799d8b
    Static pod: kube-scheduler-controlplane hash: 81d2d21449d64d5e6d5e9069a7ca99ed
    [apiclient] Found 1 Pods for label selector component=kube-scheduler
    [upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
    [upgrade/postupgrade] Applying label node-role.kubernetes.io/control-plane='' to Nodes with label node-role.kubernetes.io/master='' (deprecated)
    [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
    [kubelet] Creating a ConfigMap "kubelet-config-1.20" in namespace kube-system with the configuration for the kubelets in the cluster
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
    [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
    [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
    [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
    [addons] Applied essential addon: CoreDNS
    [addons] Applied essential addon: kube-proxy
    
    [upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.20.0". Enjoy!
    
    [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.


    ################ STEP 5 ################ 
    controlplane:~# apt list -a kubelet | grep 1.20.0
    
    WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
    
    kubelet/kubernetes-xenial 1.20.0-00 amd64

    root@controlplane:~# apt install kubelet=1.20.0-00
    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    The following held packages will be changed:
      kubelet
    The following packages will be upgraded:
      kubelet
    1 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.
    Need to get 18.8 MB of archives.
    After this operation, 4000 kB of additional disk space will be used.
    Do you want to continue? [Y/n] Y
    Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.20.0-00 [18.8 MB]
    Fetched 18.8 MB in 1s (18.1 MB/s)  
    debconf: delaying package configuration, since apt-utils is not installed
    (Reading database ... 15149 files and directories currently installed.)
    Preparing to unpack .../kubelet_1.20.0-00_amd64.deb ...
    /usr/sbin/policy-rc.d returned 101, not running 'stop kubelet.service'
    Unpacking kubelet (1.20.0-00) over (1.19.0-00) ...

    systemctl restart kubelet
</details>




<details>
  <summary markdown="span">Troubleshooting</summary>

    #### NOTE: Now it lists 2 kubeadm upgrade applys... Maybe it's that w/in 3 version release change thing
    root@controlplane:~# kubeadm upgrade plan
    [upgrade/config] Making sure the configuration is correct:
    [upgrade/config] Reading configuration from the cluster...
    [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
    [preflight] Running pre-flight checks.
    [upgrade] Running cluster health checks
    [upgrade] Fetching available versions to upgrade to
    [upgrade/versions] Cluster version: v1.19.0
    [upgrade/versions] kubeadm version: v1.20.0
    I0829 21:02:40.619651   41657 version.go:251] remote version is much newer: v1.22.1; falling back to: stable-1.20
    [upgrade/versions] Latest stable version: v1.20.10
    [upgrade/versions] Latest stable version: v1.20.10
    [upgrade/versions] Latest version in the v1.19 series: v1.19.14
    [upgrade/versions] Latest version in the v1.19 series: v1.19.14
    
    Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
    COMPONENT   CURRENT       AVAILABLE
    kubelet     2 x v1.19.0   v1.19.14
    
    Upgrade to the latest version in the v1.19 series:
    
    COMPONENT                 CURRENT   AVAILABLE
    kube-apiserver            v1.19.0   v1.19.14
    kube-controller-manager   v1.19.0   v1.19.14
    kube-scheduler            v1.19.0   v1.19.14
    kube-proxy                v1.19.0   v1.19.14
    CoreDNS                   1.7.0     1.7.0
    etcd                      3.4.9-1   3.4.9-1
    
    You can now apply the upgrade by executing the following command:
    
            kubeadm upgrade apply v1.19.14
    
    _____________________________________________________________________
    
    Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
    COMPONENT   CURRENT       AVAILABLE
    kubelet     2 x v1.19.0   v1.20.10
    
    Upgrade to the latest stable version:
    
    COMPONENT                 CURRENT   AVAILABLE
    kube-apiserver            v1.19.0   v1.20.10
    kube-controller-manager   v1.19.0   v1.20.10
    kube-scheduler            v1.19.0   v1.20.10
    kube-proxy                v1.19.0   v1.20.10
    CoreDNS                   1.7.0     1.7.0
    etcd                      3.4.9-1   3.4.13-0
    
    You can now apply the upgrade by executing the following command:
    
            kubeadm upgrade apply v1.20.10
    
    Note: Before you can perform this upgrade, you have to update kubeadm to v1.20.10.
    
    _____________________________________________________________________
    
    
    The table below shows the current state of component configs as understood by this version of kubeadm.
    Configs that have a "yes" mark in the "MANUAL UPGRADE REQUIRED" column require manual config upgrade or
    resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually
    upgrade to is denoted in the "PREFERRED VERSION" column.
    
    API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED
    kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no
    kubelet.config.k8s.io     v1beta1           v1beta1             no
    _____________________________________________________________________
</details>

### 10)  Mark the controlplane node as "Schedulable" again
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# k uncordon node/controlplane
    node/controlplane uncordoned
</details>

### 11) Next is the worker node. Drain the worker node of the workloads and mark it UnSchedulable
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# alias kdrain="k drain --ignore-daemonsets --force"
    root@controlplane:~# kdrain node/node01
    node/node01 already cordoned
    WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: default/simple-webapp-1; ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-vpm9b, kube-system/kube-proxy-h9m52
    evicting pod default/blue-746c87566d-jl5cq
    evicting pod default/blue-746c87566d-xc8ct
    evicting pod kube-system/coredns-74ff55c5b-f4gck
    evicting pod default/simple-webapp-1
    evicting pod kube-system/coredns-74ff55c5b-x7ckl
    evicting pod default/blue-746c87566d-74jjf
    evicting pod default/blue-746c87566d-fgh8x
    evicting pod default/blue-746c87566d-2w5v6
    I0829 21:20:59.483478    7020 request.go:645] Throttling request took 1.084673118s, request: GET:https://controlplane:6443/api/v1/namespaces/kube-system/pods/coredns-74ff55c5b-f4gck
    pod/blue-746c87566d-jl5cq evicted
    pod/blue-746c87566d-xc8ct evicted
    pod/blue-746c87566d-74jjf evicted
    pod/blue-746c87566d-2w5v6 evicted
    pod/blue-746c87566d-fgh8x evicted
    pod/coredns-74ff55c5b-f4gck evicted
    pod/coredns-74ff55c5b-x7ckl evicted
    pod/simple-webapp-1 evicted
    node/node01 evicted
</details>

### 12) Upgrade the worker node to the exact version v1.20.0
<details>
  <summary markdown="span">Answer</summary>

    root@node01:~# ssh node01
    root@node01:~# apt install kubeadm=1.20.0-00
    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    The following held packages will be changed:
      kubeadm
    The following packages will be upgraded:
      kubeadm
    1 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.
    Need to get 7707 kB of archives.
    After this operation, 111 kB of additional disk space will be used.
    Do you want to continue? [Y/n] Y
    Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.20.0-00 [7707 kB]
    Fetched 7707 kB in 0s (16.9 MB/s)
    debconf: delaying package configuration, since apt-utils is not installed
    (Reading database ... 15013 files and directories currently installed.)
    Preparing to unpack .../kubeadm_1.20.0-00_amd64.deb ...
    Unpacking kubeadm (1.20.0-00) over (1.19.0-00) ...
    Setting up kubeadm (1.20.0-00) ...


    root@node01:~# kubeadm upgrade node
    [upgrade] Reading configuration from the cluster...
    [upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
    [preflight] Running pre-flight checks
    [preflight] Skipping prepull. Not a control plane node.
    [upgrade] Skipping phase. Not a control plane node.
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [upgrade] The configuration for this node was successfully updated!
    [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.


    root@node01:~# apt install kubelet=1.20.0-00
    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    The following held packages will be changed:
    kubelet
    The following packages will be upgraded:
    kubelet
    1 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.
    Need to get 18.8 MB of archives.
    After this operation, 4000 kB of additional disk space will be used.
    Do you want to continue? [Y/n] Y
    Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.20.0-00 [18.8 MB]
    Fetched 18.8 MB in 1s (36.9 MB/s)
    debconf: delaying package configuration, since apt-utils is not installed
    (Reading database ... 15013 files and directories currently installed.)
    Preparing to unpack .../kubelet_1.20.0-00_amd64.deb ...
    /usr/sbin/policy-rc.d returned 101, not running 'stop kubelet.service'
    Unpacking kubelet (1.20.0-00) over (1.19.0-00) ...
    Setting up kubelet (1.20.0-00) ...
    /usr/sbin/policy-rc.d returned 101, not running 'start kubelet.service'
</details>

### 13) Mark the worker node as Schedulable again. 
<details>
  <summary markdown="span">Answer</summary>

    root@node01:~# kubectl uncordon node01 
    The connection to the server localhost:8080 was refused - did you specify the right host or port?
    root@node01:~# exit
    logout
    Connection to node01 closed.
    root@controlplane:~# k uncordon node01
    node/node01 uncordoned
</details>

### 14)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 15)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 16)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 17)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 18)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 19)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 20)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 21)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 22)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 23)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 24)
<details>
  <summary markdown="span">Answer</summary>

</details>

### 25)
<details>
  <summary markdown="span">Answer</summary>

</details>

## TABLE TEMPLATES


### 2-Col | w/ Steps
| STEP  |               |               |       
| ----- | ------------- | ------------- |
| 1     |               |               |               
| 2     |               |               |               
| 3     |               |               |               
| 4     |               |               |               
| 5     |               |               |               
| 6     |               |               |               
| 7     |               |               |               
| 8     |               |               |               
| 9     |               |               |               
| 10    |               |               |               


### 2-Col | w/ Steps
|       |               |               |       
| ----- | ------------- | ------------- |
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |               
|       |               |               |  


## #) | Section
1)
2)
3)
4)
5)
6)
7)
8)
9)
10) 
