## Questions

### Pre-Lab
COUNT="grep -vc NAME"
FIND="grep --color=always -e "^" -e"
D="--dry-run=client -oyaml"
alias km=kubeadm
alias k=kubectl
alias kg="k get"
alias kd="k describe"
alias kl="k logs"
alias kla="k label"
alias kx='k exec -it'
alias kc='k create'
alias kcf="k create -f"
alias kcd='k create $D'
alias ke="k explain"
alias kdrain="k drain --ignore-daemonsets --force"
ker() { k explain $1 --recursive=true | grep '<' | sed 's/<.*//'; }
complete -F __start_kubectl k
alias e="ETCDCTL_API=3 etcdctl" 

## NOTE

    root@controlplane:~# cat etcd-backup-and-restore.md
    1. Get etcdctl utility if it's not already present.
       go get github.com/coreos/etcd/etcdctl
    
    2. Backup
       ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
       --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
       snapshot save /opt/snapshot-pre-boot.db
    
              -----------------------------
    
              Disaster Happens
    
              -----------------------------
    3. Restore ETCD Snapshot to a new folder
       ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
       --name=master \
       --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
       --data-dir /var/lib/etcd-from-backup \
       --initial-cluster=master=https://127.0.0.1:2380 \
       --initial-cluster-token etcd-cluster-1 \
       --initial-advertise-peer-urls=https://127.0.0.1:2380 \
       snapshot restore /opt/snapshot-pre-boot.db
    
    4. Modify /etc/kubernetes/manifests/etcd.yaml
       Update --data-dir to use new target location
       --data-dir=/var/lib/etcd-from-backup
    
    Update new initial-cluster-token to specify new cluster
    --initial-cluster-token=etcd-cluster-1
    
    Update volumes and volume mounts to point to new path
    volumeMounts:
    - mountPath: /var/lib/etcd-from-backup
    name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
    name: etcd-certs
    hostNetwork: true
    priorityClassName: system-cluster-critical
    volumes:
    - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
      name: etcd-data
    - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
      name: etcd-certs



We have a working kubernetes cluster with a set of applications running. Let us first explore the setup.

### 1) How many deployments exist in the cluster?
<details> 
  <summary markdown="span">Answer</summary>

    root@controlplane:~# kg deploy | $COUNT
    2
</details>

### 2) What is the version of ETCD running on the cluster? Check the ETCD Pod or Process
<details>
  <summary markdown="span">Answer</summary>

    ==> 3.4.13

    root@controlplane:~# k exec -it etcd-controlplane -n=kube-system -- "etcdctl"
    NAME:
            etcdctl - A simple command line client for etcd3.
    
    USAGE:
            etcdctl [flags]
    
    VERSION:
            3.4.13
    
    API VERSION:
            3.4
</details>

### 3) At what address can you reach the ETCD cluster from the controlplane node?
Check the ETCD Service configuration in the ETCD POD
<details>
  <summary markdown="span">Answer</summary>

    ==> https://127.0.0.1:2379,https://10.48.143.3:2379
    
    root@controlplane:~# kd pod/etcd-controlplane -n=kube-system | grep "listen.*client"
      --listen-client-urls=https://127.0.0.1:2379,https://10.48.143.3:2379
</details>

### 4) Where is the ETCD server certificate file located?
Note this path down as you will need to use it later
<details>
  <summary markdown="span">Answer</summary>

    --cert-file=/etc/kubernetes/pki/etcd/server.crt

    root@controlplane:~# kd pod/etcd-controlplane -n=kube-system | grep cert
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
    etcd-certs:
</details>

### 5)  Where is the ETCD CA Certificate file located?
Note this path down as you will need to use it later.
<details>
  <summary markdown="span">Answer</summary>

     --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt

    root@controlplane:~# kd pod/etcd-controlplane -n=kube-system | grep ca  
    Priority Class Name:  system-node-critical
    kubernetes.io/config.hash: a275b6a8e036ea7fa03f4f8779ca2a8a
    kubernetes.io/config.mirror: a275b6a8e036ea7fa03f4f8779ca2a8a
    --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
</details>

### 6) The master nodes in our cluster are planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.
Store the backup file at location /opt/snapshot-pre-boot.db
<details>
  <summary markdown="span">Answer</summary>

    root@controlplane:~# alias e='ETCDCTL_API=3 etcdctl'

    root@controlplane:~# e snapshot save $ETCD_ARGS /opt/snapshot-pre-boot.db
    Snapshot saved at /opt/snapshot-pre-boot.db


    NOTE: If you see "Error: rpc error: code = Unavailable desc = transport is closing", you need to specify CERT,CA, & KEY
    root@controlplane:~# e snapshot save /opt/snapshot-pre-boot.db
    Error: rpc error: code = Unavailable desc = transport is closing
</details>

### 7) Filler Question
<details>
  <summary markdown="span">Answer</summary>

</details>

### 8) Wake up! We have a conference call! After the reboot the master nodes came back online, but none of our applications are accessible. Check the status of the applications on the cluster. What's wrong?
1) Deployments are not present
2) Services are not present
3) All of the above
4) Pods are not present
<details>
  <summary markdown="span">Answer</summary>

    ==> 3

    root@controlplane:~# k get all   
    NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
    service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   95s
</details>

### 9) Luckily we took a backup. Restore the original state of the cluster using the backup file.
1) Deployments: 2
2) Services: 3
<details>
  <summary markdown="span">Answer</summary>
    
    
    ## Step 1: Restore Etcd Snapshot to specified "--data-dir" location
    root@controlplane:~# ETCD_ARGS="--key=/etc/kubernetes/pki/etcd/server.key --cert=/etc/kubernetes/pki/etcd/server.crt --cacert=/etc/kubernetes/pki/etcd/ca.crt"
    root@controlplane:~# e snapshot restore $ETCD_ARGS --data-dir=/var/lib/etcd-backup /opt/snapshot-pre-boot.db
    
    ## Step 2: Change the location (NOTE, Cat out 1st, then cat back to same dir)
    root@controlplane:/# kg all
    NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
    service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3m18s
    root@controlplane:/# STR="/var/lib/etcd" 
    root@controlplane:/# sed "s;$STR;$STR-backup;" -i /etc/kubernetes/manifests/etcd.yaml
    root@controlplane:/# kg all | grep -vc NAME
    15
        
</details>
